# -*- coding: utf-8 -*-
"""Analysis-RibonanzaNet-SM_data_split.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hNuHvWO-telhhzOdd6plakA9s4cw6H0F
"""

import pandas as pd
import torch
import matplotlib.pyplot as plt
import numpy as np
import torch
import random
import json
import pandas as pd
from collections import defaultdict
import random
import re

#set seed for everything
torch.manual_seed(0)
np.random.seed(0)
random.seed(0)

train_data = pd.read_json('/sm_data/train_data__with_SN_SM.json')

train= pd.read_json('/sm_data/SM_train_clustered.json')

# Function to parse the .clstr file and extract clusters
def parse_clstr_file(clstr_file):
    clusters = defaultdict(list)  # Dictionary to hold clusters
    with open(clstr_file, 'r') as f:
        current_cluster = None
        for line in f:
            line = line.strip()

            # Identify the cluster line, e.g., ">Cluster 122"
            if line.startswith('>Cluster'):
                current_cluster = line.split()[1]  # Capture cluster number (e.g., '122')

            # Capture sequence IDs from lines starting with a number (e.g., "0       115aa, >e09e90e3166c... *")
            elif line and line[0].isdigit():
                match = re.search(r">([a-zA-Z0-9]+)", line)  # Regex to extract the sequence ID
                if match:
                    seq_id = match.group(1)
                    clusters[current_cluster].append(seq_id)

    return clusters

# Function to manually split clusters into training and validation sets
def split_clusters(clusters, test_size=0.15):
    # Shuffle clusters to avoid bias
    cluster_ids = list(clusters.keys())
    random.shuffle(cluster_ids)

    # Determine the number of clusters to use for validation (15%)
    num_val_clusters = int(len(cluster_ids) * test_size)

    # Create train and validation cluster lists
    val_clusters = cluster_ids[:num_val_clusters]
    train_clusters = cluster_ids[num_val_clusters:]

    # Create lists of sequence IDs for training and validation sets
    train_seq_ids = [seq_id for cluster in train_clusters for seq_id in clusters[cluster]]
    val_seq_ids = [seq_id for cluster in val_clusters for seq_id in clusters[cluster]]

    return train_seq_ids, val_seq_ids

# Function to load training data from a JSON file
def load_training_data(json_file):
    with open(json_file, 'r') as f:
        data = json.load(f)

    # Convert data to pandas DataFrame for easier manipulation
    df = pd.DataFrame(data)
    return df

# Function to save the split data into new JSON files
def save_split_data(train_data, val_data, train_file, val_file):
    with open(train_file, 'w') as f:
        json.dump(train_data, f, indent=4)

    with open(val_file, 'w') as f:
        json.dump(val_data, f, indent=4)

# Main code execution
def main(clstr_file, json_file, train_file, val_file):
    # Step 1: Parse the .clstr file to get clusters
    clusters = parse_clstr_file(clstr_file)

    # Step 2: Load the training data from JSON
    df = load_training_data(json_file)

    # Step 3: Create the sequence list and group IDs
    seq_ids = df['id'].values
    group_ids = np.array([cluster_id for cluster_id in clusters for seq in clusters[cluster_id] if seq in seq_ids])

    # Step 4: Perform the manual split based on clusters
    train_seq_ids, val_seq_ids = split_clusters(clusters)

    # Step 5: Filter the train and validation data
    train_data = df[df['id'].isin(train_seq_ids)].to_dict(orient='records')
    val_data = df[df['id'].isin(val_seq_ids)].to_dict(orient='records')

    # Step 6: Save the resulting train and validation data to JSON files
    save_split_data(train_data, val_data, train_file, val_file)
    print(f"Training data saved to {train_file}")
    print(f"Validation data saved to {val_file}")

# Path to your files
clstr_file = "/train_rnet_sm_75_5.clstr"  # Replace with the actual path to your .clstr file
json_file = "/sm_data/train_data__with_SN_SM.json"  # Replace with the path to your training JSON file
train_file = "SM_train_data.json"  # Path to save the train data
val_file = "SM_val_data.json"  # Path to save the validation data

# Execute the main function
main(clstr_file, json_file, train_file, val_file)

#Check the length of the train and val data
train = pd.read_json('/sm_data/SM_train_data.json')
val = pd.read_json('/sm_data/SM_val_data.json')
print(f'The training data is {len(train)} long')
print(f'The validation data is {len(val)} long')

train_sequences = train['sequence'].to_list()
print(train_sequences)
val_sequences = val['sequence'].to_list()
print(val_sequences)

drug_list = ['Argi', 'Eryt', 'Kana', 'Mito', 'Paro', 'Spec', 'Tetr', 'NoDr']
for drug in drug_list:
  df = pd.read_json(f'/abs_reac_orig_train_test_split/{drug}_train_no_clipped.json')
  train_data = df[df['sequence'].isin(train_sequences)]
  train_data.to_json(f'SM_{drug}_train.json')
  val_data = df[df['sequence'].isin(val_sequences)]
  val_data.to_json(f'SM_{drug}_val.json')
  print(f'Train file for {drug} saved to SM_{drug}_train.json, and Val file for {drug} saved to SM_{drug}_val.json')

argi_testing = pd.read_json('/SM_Argi_train.json')
print(len(argi_testing))
print(argi_testing.columns)

mito_testing = pd.read_json('/SM_Mito_train.json')
print(len(mito_testing))
print(mito_testing.columns)

"""Now I am going to split the train data reactivity difference metadata file"""

train_reac_diff = pd.read_json('/sm_data/train_data_reac_diff_with_abs_nodr.json')

train_reac_diff.columns

len(train_reac_diff)

train_reac_diff_data = train_reac_diff[train_reac_diff['sequence'].isin(train_sequences)]

train_reac_diff_data.to_json('SM_train_reac_diff_with_NoDr_abs_reac.json')
val_reac_diff_data = train_reac_diff[train_reac_diff['sequence'].isin(val_sequences)]
val_reac_diff_data.to_json(f'SM_val_reac_diff_with_NoDr_abs_reac.json')

sm_train_reac_diff_load = pd.read_json('/SM_train_reac_diff.json')
sm_val_reac_diff_load = pd.read_json('/SM_val_reac_diff.json')

print(f'original sm_train_reac_diff_columns = {sm_train_reac_diff_load.columns}')
print(f'original sm_val_reac_diff_columns = {sm_val_reac_diff_load.columns}')


sm_train_reac_diff_load['NoDr'] = train['NoDr']
sm_train_reac_diff_load['NoDr_SN'] = train['NoDr_SN']

sm_val_reac_diff_load['NoDr'] = val['NoDr']
sm_val_reac_diff_load['NoDr_SN'] = val['NoDr_SN']

print(f'new sm_train_reac_diff_columns = {sm_train_reac_diff_load.columns}')
print(f'new sm_val_reac_diff_columns = {sm_val_reac_diff_load.columns}')

